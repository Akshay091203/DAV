{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshay091203/DAV/blob/main/DAV_EXP_7_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd8YA9c3m_H0"
      },
      "source": [
        "# **Aim : Perform the steps involved in Text Analytics in Python & R**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YoOcs1xnV7L"
      },
      "source": [
        "# **Text Analytics in Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_vcdZpandQY"
      },
      "source": [
        "**1. Tokenization (Sentence & Word)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8T9bVbAm7nK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18baa4c5-8b73-4a25-936c-8d44325b08ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['This is a sample sentence.', 'Tokenization is important for NLP.']\n",
            "Words: ['This', 'is', 'a', 'sample', 'sentence', '.', 'Tokenization', 'is', 'important', 'for', 'NLP', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncM66LEqn49j"
      },
      "source": [
        "**2. Frequency Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuTu4Of1vZNp",
        "outputId": "a6444ba2-2a52-438e-9e68-3e410e1870fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is: 2\n",
            ".: 2\n",
            "This: 1\n",
            "a: 1\n",
            "sample: 1\n"
          ]
        }
      ],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Create a frequency distribution of the words in the text\n",
        "freq_dist = FreqDist(words)\n",
        "\n",
        "# Print the most common words and their frequencies\n",
        "for word, frequency in freq_dist.most_common(5):\n",
        "    print(f\"{word}: {frequency}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OE9Evtan2Z-",
        "outputId": "54076ddb-268a-49b4-c975-74ea53cdebfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word frequency: <FreqDist with 10 samples and 12 outcomes>\n"
          ]
        }
      ],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "word_freq = FreqDist(words)\n",
        "print(\"Word frequency:\", word_freq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA2dhUeXocbE"
      },
      "source": [
        "**3. Remove stopwords & punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2yXofSkoYlI",
        "outputId": "0da060fc-b2a9-47f7-d96e-4c401ec099b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence', 'Tokenization', 'important', 'NLP']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Remove punctuations\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "words = [word for word in words if word not in punctuations]\n",
        "\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQbIh5nopBLq"
      },
      "source": [
        "**4. Lexicon Normalization (Stemming, Lemmatization)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80DA5XBCph_u",
        "outputId": "6158aac9-3e14-4558-8d86-ab919ec9102b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['thi', 'is', 'a', 'sampl', 'sentenc', '.', 'token', 'is', 'import', 'for', 'nlp', '.']\n",
            "Lemmatized words: ['This', 'is', 'a', 'sample', 'sentence', '.', 'Tokenization', 'is', 'important', 'for', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Assuming 'text' contains the input text\n",
        "text = \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Instantiate the Porter Stemmer and WordNet Lemmatizer\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform stemming and lemmatization\n",
        "stemmed_words = [ps.stem(word) for word in words]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCt3MVI1p_KK"
      },
      "source": [
        "**5. Part of Speech tagging**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm1kXenpI_Nw",
        "outputId": "5aa90995-816a-4edf-8a70-7072048de18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH1NN8ETqN3O",
        "outputId": "5beb9ce0-48a2-4fd8-8a60-beba45b70306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part Of Speech Tagging: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.'), ('Tokenization', 'NN'), ('is', 'VBZ'), ('important', 'JJ'), ('for', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "print(\"Part Of Speech Tagging:\", tagged_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ElNFDPqeIp"
      },
      "source": [
        "**6. Named Entity Recognization**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_iuE9XiJK7H",
        "outputId": "bc2d3052-82bc-4541-8756-9f017d88dc80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYBy5m1Lqdwa",
        "outputId": "b69667d2-d1fb-4caa-f99f-40a05ccfacab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Honolulu/NNP)\n",
            "  ,/,\n",
            "  (GPE Hawaii/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = \"Barack Obama was born in Honolulu, Hawaii.\"\n",
        "words = word_tokenize(text)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "ne_tree = nltk.ne_chunk(tagged_words)\n",
        "print(ne_tree)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXduEXItq70_"
      },
      "source": [
        "**7. Scrape data from a website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa0LipRnrFB8",
        "outputId": "5f67e87a-93b1-4016-a760-beb900e5300c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text data from website: \n",
            "\n",
            "\n",
            "Example Domain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Example Domain\n",
            "This domain is for use in illustrative examples in documents. You may use this\n",
            "    domain in literature without prior coordination or asking for permission.\n",
            "More information...\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://example.com'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "text_data = soup.get_text()\n",
        "print(\"Text data from website:\", text_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GqUrN2lsVWK"
      },
      "source": [
        "# **Text Analytics in R**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EYnSpp2sZOR"
      },
      "source": [
        "**1. Tokenization (Sentence & Word)**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization (Sentence & Word)\n",
        "text <- \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences <- strsplit(text, \"\\\\.\")[[1]]\n",
        "words <- unlist(strsplit(text, \"\\\\s+\"))\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print(\"Words:\")\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoC9nJigDZt9",
        "outputId": "91b48743-7fc0-4410-b75f-2a27db6ec6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentences:\"\n",
            "[1] \"This is a sample sentence\"          \" Tokenization is important for NLP\"\n",
            "[1] \"Words:\"\n",
            " [1] \"This\"         \"is\"           \"a\"            \"sample\"       \"sentence.\"   \n",
            " [6] \"Tokenization\" \"is\"           \"important\"    \"for\"          \"NLP.\"        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")\n",
        "library(tokenizers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ6KMJD8Jx8i",
        "outputId": "d5c6998f-4c8a-4731-a2d1-34507f8d06cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMK6zkKvscTW",
        "outputId": "9d6afde9-44b5-45eb-d574-dd115fe8e684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentences:\"\n",
            "[[1]]\n",
            "[1] \"This is a sample sentence.\"         \"Tokenization is important for NLP.\"\n",
            "\n",
            "[1] \"Words:\"\n",
            "[[1]]\n",
            " [1] \"this\"         \"is\"           \"a\"            \"sample\"       \"sentence\"    \n",
            " [6] \"tokenization\" \"is\"           \"important\"    \"for\"          \"nlp\"         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text <- \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences <- tokenize_sentences(text)\n",
        "words <- tokenize_words(text)\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print(\"Words:\")\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytwKTFo2suc4"
      },
      "source": [
        "**2. Frequency Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Distribution\n",
        "word_freq <- table(words)\n",
        "print(\"Word frequency:\")\n",
        "print(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjjNOQNyDnTc",
        "outputId": "1bc84c61-6705-4434-bfe4-1d0c50b86ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Word frequency:\"\n",
            "words\n",
            "           a          for    important           is         NLP.       sample \n",
            "           1            1            1            2            1            1 \n",
            "   sentence.         This Tokenization \n",
            "           1            1            1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OwmHXrZsuZU"
      },
      "source": [
        "**3. Remove stopwords & punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords & punctuations\n",
        "stop_words <- c(\"is\", \"a\", \"for\")  # Example list of stopwords\n",
        "filtered_words <- words[!tolower(words) %in% stop_words & !grepl(\"[[:punct:]]\", words)]\n",
        "print(\"Filtered words:\")\n",
        " print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29vIl9mVDq5F",
        "outputId": "a59c5556-1f3a-476e-b2b0-73020cbf3f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Filtered words:\"\n",
            "[1] \"This\"         \"sample\"       \"Tokenization\" \"important\"   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"udpipe\")"
      ],
      "metadata": {
        "id": "-Px-IVJ_QA9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OjLYNhcsuW8"
      },
      "source": [
        "**4. Lexicon Normalization (Stemming, Lemmatization)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexicon Normalization (Stemming, Lemmatization)\n",
        "# For Stemming (using udpipe)\n",
        "library(udpipe)\n",
        "ud_model <- udpipe_download_model(language = \"english\")\n",
        "ud_model <- udpipe_load_model(ud_model$file_model)\n",
        "x <- udpipe_annotate(ud_model, x = filtered_words, doc_id = 1:length(filtered_words))  # Specify the 'x' argument\n",
        "stemmed_words <- as.data.frame(x)$lemma\n",
        "\n",
        "# For Lemmatization (using udpipe)\n",
        "x <- udpipe_annotate(ud_model, x = filtered_words, doc_id = 1:length(filtered_words), lemma = TRUE)  # Specify the 'lemma' option\n",
        "lemmatized_words <- as.data.frame(x)$lemma\n",
        "\n",
        "print(\"Stemmed words:\")\n",
        "print(stemmed_words)\n",
        "print(\"Lemmatized words:\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "VYG_kbpVJ7g0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbeeb44-d836-4db0-a941-cfa1d8ad9f8f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /content/english-ewt-ud-2.5-191206.udpipe\n",
            "\n",
            " - This model has been trained on version 2.5 of data from https://universaldependencies.org\n",
            "\n",
            " - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0\n",
            "\n",
            " - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.\n",
            "\n",
            " - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')\n",
            "\n",
            "Downloading finished, model stored at '/content/english-ewt-ud-2.5-191206.udpipe'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Stemmed words:\"\n",
            " [1] \"this\"         \"be\"           \"a\"            \"sample\"       \"sentence\"    \n",
            " [6] \"Tokenization\" \"be\"           \"important\"    \"for\"          \"NLP\"         \n",
            "[1] \"Lemmatized words:\"\n",
            " [1] \"this\"         \"be\"           \"a\"            \"sample\"       \"sentence\"    \n",
            " [6] \"Tokenization\" \"be\"           \"important\"    \"for\"          \"NLP\"         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_pzDNqjsuUh"
      },
      "source": [
        "**5. Part of Speech tagging**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of Speech tagging using udpipe library\n",
        "library(udpipe)\n",
        "\n",
        "# Load the UDPipe model\n",
        "ud_model <- udpipe_download_model(language = \"english\")\n",
        "ud_model <- udpipe_load_model(ud_model$file_model)\n",
        "\n",
        "# Define your filtered text data in the variable filtered_words\n",
        "filtered_words <- c(\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"Tokenization\", \"is\", \"important\", \"for\", \"NLP\")\n",
        "\n",
        "# Annotate the text\n",
        "x <- udpipe_annotate(ud_model, x = filtered_words, doc_id = 1:length(filtered_words))\n",
        "\n",
        "# Extract the Part-of-Speech tags\n",
        "pos_tags <- as.data.frame(x)$upos\n",
        "\n",
        "# Print the Part-of-Speech tags\n",
        "print(\"POS tagging:\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "Q0w4raFNEZ4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c77d0d7-ccbe-437a-b20e-52faa53512c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /content/english-ewt-ud-2.5-191206.udpipe\n",
            "\n",
            " - This model has been trained on version 2.5 of data from https://universaldependencies.org\n",
            "\n",
            " - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0\n",
            "\n",
            " - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.\n",
            "\n",
            " - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')\n",
            "\n",
            "Downloading finished, model stored at '/content/english-ewt-ud-2.5-191206.udpipe'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"POS tagging:\"\n",
            " [1] \"PRON\" \"AUX\"  \"DET\"  \"NOUN\" \"NOUN\" \"NOUN\" \"AUX\"  \"ADJ\"  \"ADP\"  \"SYM\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLCRx7sOsuRm"
      },
      "source": [
        "**6. Named Entity Recognization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "12PH6X3ztgE2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "0378e2a6-0ac8-4e85-ddc1-635336fe561c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘openNLPdata’, ‘rJava’\n",
            "\n",
            "\n",
            "Warning message in install.packages(\"openNLP\"):\n",
            "“installation of package ‘rJava’ had non-zero exit status”\n",
            "Warning message in install.packages(\"openNLP\"):\n",
            "“installation of package ‘openNLPdata’ had non-zero exit status”\n",
            "Warning message in install.packages(\"openNLP\"):\n",
            "“installation of package ‘openNLP’ had non-zero exit status”\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error in library(openNLP): there is no package called ‘openNLP’\n",
          "traceback": [
            "Error in library(openNLP): there is no package called ‘openNLP’\nTraceback:\n",
            "1. library(openNLP)"
          ]
        }
      ],
      "source": [
        "install.packages(\"NLP\")\n",
        "install.packages(\"openNLP\")\n",
        "library(openNLP)\n",
        "library(NLP)\n",
        "\n",
        "ner_tags <- maxent_tagger_chunker(filtered_words, pos_tags)\n",
        "print(\"Named Entities:\")\n",
        "print( ner_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oUPp2QcsuIl"
      },
      "source": [
        "**7. Scrape data from a website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6ECqdvU-thCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ed6f20-c44e-472d-93d3-aed8b0a146ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Text data from website:\"\n",
            "[1] \"Example Domain\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \\\"Segoe UI\\\", \\\"Open Sans\\\", \\\"Helvetica Neue\\\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    \\n\\n    Example Domain\\n    This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\n    More information...\\n\\n\"\n"
          ]
        }
      ],
      "source": [
        "install.packages(\"rvest\")\n",
        "library(rvest)\n",
        "\n",
        "url <- \"https://example.com\"\n",
        "page <- read_html(url)\n",
        "text_data <- page %>%\n",
        "  html_text()\n",
        "\n",
        "print(\"Text data from website:\")\n",
        "print(text_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmMRreRZstxD"
      },
      "source": [
        "# **Outcome :**\n",
        "1. Identified the Text Analytics Libraries in Python and R\n",
        "2. Performed simple experiments with these libraries in Python and R"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}